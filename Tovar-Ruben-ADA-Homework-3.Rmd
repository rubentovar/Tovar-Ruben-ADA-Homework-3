---
title: "Tovar-Ruben-ADA-Homework-3"
author: "Ruben Tovar"
date: "5/10/2020"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
###########CHALLENGE 1 ################
```{r}
library(tidyverse)

library(curl)

library(dplyr)

read <- curl("https://raw.githubusercontent.com/difiore/ADA-datasets/master/KamilarAndCooperData.csv")

data <- read.csv(read, header = TRUE, sep = ",", stringsAsFactors = FALSE)

head(data)
```
###Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot.
```{r}
(plotlm <- ggplot(lm, aes(Brain_Size_Species_Mean, MaxLongevity_m)) + geom_point() + geom_smooth(method="lm", se=FALSE))
(plotlog_lm <- ggplot(lm, aes(log(Brain_Size_Species_Mean), log(MaxLongevity_m))) + geom_point() + geom_smooth(method="lm", se=FALSE))
```
### Fit regression model
```{r}
lm <- lm(Brain_Size_Species_Mean ~ MaxLongevity_m, data)

summary(lm)

data <- data %>% 
  mutate(log_BMASSFM = (log(Body_mass_female_mean)),
  logMaxLongevity = (log(MaxLongevity_m)))

loglm <-lm(log_BMASSFM ~ logMaxLongevity, data)

summary(loglm)
```
###Identify and interpret the point estimate of the slope & outcome of the test associated with the hypotheses 
```{r}
nanodata<-na.omit(data)

bss<-nanodata$Brain_Size_Species_Mean

lgv<-nanodata$MaxLongevity_m

(pointestimate_slope <- cor(bss, lgv) * (sd(lgv) / sd(bss)))
```
###Find a 90% CI for the slope
```{r}
alpha <- 0.10

(CI <- confint(lm, level = 1 - alpha))

(CIlog <- confint(loglm, level = 1 - alpha))
```
###Using your model, add lines for the 90% confidence and prediction interval bands on the plot, and add a legend to differentiate between the lines.
```{r}
library(broom)

y.line <- predict(loglm, newdata = data.frame(logMaxLongevity = data$logMaxLongevity))
df <- data.frame(cbind(data$logMaxLongevity, data$log_BMASSFM, y.line))
names(df) <- c("x", "y", "yline")
df <- augment(loglm)
```
###Intervals
```{r}
df <- df %>%
  mutate(
    c.lwr = .fitted - qt(1 - alpha / 2, nrow(df) - 2) * .se.fit,
    c.upr = .fitted + qt(1 - alpha / 2, nrow(df) - 2) * .se.fit
  )
```
###sd deviation
```{r}
sd <- glance(loglm) %>% pull(sigma)
```
###CI Predicted values
```{r}
df <- df %>%
mutate(
se.prediction = sqrt(sd^2 + .se.fit^2),
p.lwr = .fitted - qt(1 - alpha / 2, nrow(df) - 2) * se.prediction,
p.upr = .fitted + qt(1 - alpha / 2, nrow(df) - 2) * se.prediction
)
head(df)

g <- ggplot(data = data, aes(x = log_BMASSFM, y = logMaxLongevity))
g <- g + geom_point()
g <- g + geom_line(data = df,aes(x = log_BMASSFM, y = .fitted, color = "black"), na.rm = TRUE) 
g <- g + geom_line(data = df,aes(x = log_BMASSFM, y = c.lwr, color = "orange"), na.rm = TRUE) 
g <- g + geom_line(data = df, aes(x = log_BMASSFM, y = c.upr, color = "orange"), na.rm = TRUE) 
g <- g + geom_line(data = df,aes(x = log_BMASSFM, y = p.lwr, color = "green"), na.rm = TRUE) 
g <- g + geom_line(data = df,aes(x = log_BMASSFM, y = p.upr, color = "green"), na.rm = TRUE) 
g 

g2 <- ggplot(data = data, aes(x = log_BMASSFM, y = logMaxLongevity), alpha = 0.5) +
  geom_point(na.rm = TRUE) +
```
##add regression line
```{r}
  geom_line(
    data = df, aes(x = log_BMASSFM, y = .fitted), color = "black",
    lwd = 1
  ) +
```
##add a ribbon layer
```{r}
  geom_ribbon(
    data = df, aes(x = log_BMASSFM, ymin = c.lwr, ymax = c.upr, colour = "Confidence Interval"),
    #with transparency set to 0.2
    alpha = 0.2,
    fill = "grey"
  ) +
  geom_ribbon(
    data = df, aes(x = log_BMASSFM, ymin = p.lwr, ymax = p.upr, colour = "Prediction Interval"),
    # ... with transparency set to 0.2
    alpha = 0.2,
    fill = "blue"
  )
g2
```
###Produce a point estimate and associated 90% prediction interval for the longevity of a species whose brain weight is 750 gm.
```{r}
(b750 <- 248.9523 + (1.2180 * 750))
```
###Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?

Yes, there seems to me a well fitted linear relationship based on the graph.

###Looking at your two models (i.e., untransformed versus log-log transformed), which do you think is better? Why?

Log transformed helps with the noise of the variation.


########CHALLENGE 2############

###Using the “KamilarAndCooperData.csv” dataset, run a linear regression looking at log(HomeRange_km2) in relation to log(Body_mass_female_mean) and report your  
β coeffiecients (slope and intercept).
```{r}
data <- data %>% 
  mutate(log_BMASSFM = log(Body_mass_female_mean),
  loghomie = (log(HomeRange_km2)))

HomieRangeLM <- lm(log_BMASSFM ~ loghomie, data)

summary(HomieRangeLM)
```
###use bootstrapping to sample from the dataset 1000 times with replacement, each time fitting the same model and calculating the appropriate coefficients.
```{r}
library(infer)

set.seed(213)
strapnboots <- data %>%
specify(log_BMASSFM ~ logHOMER) %>%
generate(reps = 1000, type = "bootstrap") %>%
calculate(stat = "slope")
```
###Histogram
```{r}
hist(strapnboots$stat,
  main = "Histogram of Bootstrapped\nSlope Values",
  xlab = "Slope Coefficient"
)
```
###Bootstrap
```{r}
strapnboots <- data %>%
  specify(log_BMASSFM ~ logHOMER) %>%
  generate(reps = 1000, type = "bootstrap") 
  
slope <- vector()

intercept <- vector()

for (i in 1:213) {
  bootreps <- filter(strapnboots, replicate == i)
  bootrepsB <- lm(log_BMASSFM ~ logHOMER, data = bootreps)
  slope[[i]] <- bootrepsB$coefficients[[2]]
  intercept[[i]] <- bootrepsB$coefficients[[1]]
  }
```
# store data
```{r}
BR_Boot <- tibble(slope = slope, intercept = intercept)
```
###Plot a histogram of these sampling distributions for  
β0 and  β1.
```{r}
hist(BR_Boot$slope,
main = "Histogram of Slope Values",
xlab = "Slope")

hist(BR_Boot$intercept,
 main = "Histogram of Intercept Values",
xlab = "Intercept")

alpha <- 0.05

confidence_level <- 1 - alpha

p_lower <- alpha / 2

p_upper <- 1 - (alpha / 2)

degrees_of_freedom <- nrow(BR_Boot) - 2

critical_value <- qt(p_upper, df = degrees_of_freedom)

###B1

BRB_ErrorA <- BR_Boot %>% 
  summarize(
    estimate = mean(slope),
    std.error = sd(slope),
    lower = estimate - std.error * critical_value,
    upper = estimate + std.error * critical_value,
    boot.lower = quantile(slope, p_lower),
    boot.upper = quantile(slope, p_upper)
    )
    
BRB_ErrorA

###B0

BRB_ErrorO <- BR_Boot %>% 
  summarize(
    estimate = mean(intercept),
    std.error = sd(intercept),
    lower = estimate - std.error * critical_value,
    upper = estimate + std.error * critical_value,
    boot.lower = quantile(slope, p_lower),
    boot.upper = quantile(slope, p_upper)
    )

BRB_ErrorO
```
###How do the SEs estimated from the bootstrap sampling distribution compare to those estimated mathematically as part of lm() function?

*Both the intercept and slope changed for lm()

###How do you bootstrap CIs compare to those estimated mathematically as part of the lm() function?
```{r}
alpha = 0.05

(CI <- confint(HomieRangeLM, level = 1 - alpha))
```
*2.5 %    97.5 %
(Intercept) 8.2844504 8.6878632
loghomie    0.4243665 0.5883935

*Both the orginal and bootstrap values are comparable.

################## Challenge 3 #####################

###Write your own function, called boot_lm(), that takes as its arguments a dataframe (d=), a linear model (model=, written as a character string, e.g., “logHR ~ logBM”), a user-defined confidence interval level (conf.level=, with default “0.95”), and a number of bootstrap replicates (reps=, with default “1000”).
```{r}
boot_lm <- function(d, model, conf.level=0.95, reps=1000){
d <- d %>% 
mutate(log_HomeRange = log(HomeRange_km2),
log_BMASSFM = log(Body_mass_female_mean),
log_DayLength = log(DayLength_km),
log_MGS = log(MeanGroupSize))


model <- as.formula(model)
fit <- lm(model, data = d)
```
##Boot strap
```{r}
strapnboots <- d %>%
generate(reps = reps, type = "bootstrap") 
slope <- vector()
intercept <- vector()

for (i in 1:213) {
Range_Boot <- filter(strapnboots, replicate == i)
Range_Bootlm <- lm(model, Range_Boot)
slope[[i]] <- Range_Bootlm$coefficients[[2]]
intercept[[i]] <- Range_Bootlm$coefficients[[1]]
}
```
##Vectors
```{r}
Range_Boot <- tibble(slope = slope, intercept = intercept)

#Alpha, CI boundaries, and critical values
alpha <- 0.05
confidence_level <- 1 - alpha
p_lower <- alpha / 2
p_upper <- 1 - (alpha / 2)
degrees_of_freedom <- nrow(Range_Boot) - 2
critical_value <- qt(p_upper, df = degrees_of_freedom)
```
##β1
```{r}
Range_Bootlm_ErrorA <- Range_Boot %>% 
summarize(
estimate = mean(slope),
std.error = sd(slope),
lower = estimate - std.error * critical_value,
upper = estimate + std.error * critical_value,
boot.lower = quantile(slope, p_lower),
boot.upper = quantile(slope, p_upper)
)

Range_Bootlm_ErrorA
```
##β0
```{r}
Range_Bootlm_ErrorO <- Range_Boot %>% 
summarize(
estimate = mean(intercept),
std.error = sd(intercept),
lower = estimate - std.error * critical_value,
upper = estimate + std.error * critical_value,
boot.lower = quantile(slope, p_lower),
boot.upper = quantile(slope, p_upper)
)

Range_Bootlm_ErrorO
```
# combined
```{r}
C <-rbind(Range_Bootlm_ErrorA,Range_Bootlm_ErrorO)
C
}
```
## log(HomeRange_km2) ~ log(Body_mass_female_mean)
```{r}
boot_lm(d = data, model = "log_HomeRange ~ log_BMASSFM")
```
## log(DayLength_km) ~ log(Body_mass_female_mean)
```{r}
boot_lm(d = data, model = "log_DayLength ~ log_BMASSFM")
```
## log(HomeRange_km2) ~ log(Body_mass_female_mean) + MeanGroupSize
```{r}
boot_lm(d = data, model = "log_HomeRange ~ log_BMASSFM + log_MGS")
```






